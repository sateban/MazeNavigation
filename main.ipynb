{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "(270, 480, 4)\n",
      "Object Detected\n",
      "(270, 480, 4)\n",
      "Object Detected\n",
      "(270, 480, 4)\n",
      "Object Detected\n",
      "(270, 480, 4)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def get_screen_size():\n",
    "\tuser32 = ctypes.windll.user32\n",
    "\tscreensize = (user32.GetSystemMetrics(0), user32.GetSystemMetrics(1))\n",
    "\treturn screensize\n",
    "\n",
    "def combineImage(baseImage, overlayImage):\n",
    "\t# Load the JPG image\n",
    "\tjpg_image = Image.open(baseImage)\n",
    "\n",
    "\t# Load the PNG image with transparency\n",
    "\tpng_image = Image.open(overlayImage)\n",
    "\n",
    "\t# Ensure both images have the same mode and alpha channel\n",
    "\tjpg_image = jpg_image.convert('RGBA')\n",
    "\tpng_image = png_image.convert('RGBA')\n",
    "\n",
    "\t# Resize the PNG image to fit onto the JPG image\n",
    "\tpng_image = png_image.resize(jpg_image.size, Image.LANCZOS)\n",
    "\n",
    "\t# Composite the images, using the alpha channel of the PNG image as a mask\n",
    "\tresult_image = Image.alpha_composite(jpg_image, png_image)\n",
    "\n",
    "\t# Save or display the result\n",
    "\tresult_image.show()\n",
    "\tresult_image.save('./samples/overlayed_image.jpg')\n",
    "\n",
    "def removeBlack(image):\t\n",
    "\t# filename = './samples/maze.png'\n",
    "\t# image = cv2.imread(filename)\n",
    "\t# Convert the image to the HSV color space for better color manipulation\n",
    "\t# hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\t# image = image.reshape(image.shape + (1,))\n",
    "\thsv_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "\t# Define the lower and upper bounds for the black color in HSV\n",
    "\t# lower_black = np.array([0, 0, 0], dtype=np.uint8)\n",
    "\t# upper_black = np.array([180, 255, 30], dtype=np.uint8)\n",
    "\n",
    "\t# # Create a mask for the black color in the image\n",
    "\t# black_mask = cv2.inRange(hsv_image, lower_black, upper_black)\n",
    "\n",
    "\t# # Invert the mask so that black becomes white and vice versa\n",
    "\t# inverse_mask = cv2.bitwise_not(black_mask)\n",
    "\n",
    "\t# Create a 4-channel image (BGRA) with an alpha channel\n",
    "\th, w = image.shape[:2]\n",
    "\tbgra_image = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "\n",
    "\t# Copy the RGB channels from the original image to the BGRA image\n",
    "\t# bgra_image[:, :, :3] = image\n",
    "\t# bgra_image[:, :, :3] = [255, 0, 0] # Change the color of White to other \n",
    "\tbgra_image[:, :, :3][image != 0] = [255, 0, 0]\n",
    "\t\n",
    "\tbgra_image[:, :, 3] = image\n",
    "\n",
    "\t# cv2.imshow(\"Corners\", bgra_image)\n",
    "\n",
    "\t# Set the alpha channel based on the inverse mask\n",
    "\t# bgra_image[:, :, 3] = inverse_mask\n",
    "\n",
    "\treturn bgra_image\n",
    "\n",
    "def detect_leading_lines(leadFrame):\n",
    "\t# Convert the frame to grayscale\n",
    "\t# gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\n",
    "\t# Apply GaussianBlur to reduce noise and help Canny edge detection\n",
    "\tblurred = cv2.GaussianBlur(leadFrame, (5, 5), 0)\n",
    "\t\n",
    "\t# Apply Canny edge detection\n",
    "\tedges = cv2.Canny(blurred, 50, 150)\n",
    "\t\n",
    "\t# Apply Hough Line Transform to detect lines in the image\n",
    "\tlines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=50, minLineLength=50, maxLineGap=30)\n",
    "\t\n",
    "\t# Check if there are lines and analyze them\n",
    "\t# Detects any line positionss\n",
    "\t# if lines is not None:\n",
    "\t#     for line in lines:\n",
    "\t#         x1, y1, x2, y2 = line[0]\n",
    "\t#         # You can perform further analysis or filtering based on the slope, position, etc.\n",
    "\t#         # For simplicity, let's just draw the lines on the original leadFrame\n",
    "\t#         cv2.line(leadFrame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "\t# lower_line_y_coordinates = []\n",
    "\t# frame_height = leadFrame.shape[0]\n",
    "\t# max_avg = 0\n",
    "\n",
    "\t# if lines is not None:\n",
    "\t# \tfor line in lines:\n",
    "\t# \t\tx1, y1, x2, y2 = line[0]\n",
    "\t# \t\t# print(f'max(y1, y2): {max(y1, y2)}')\n",
    "\t# \t\tmax_avg += max(y1, y2)\n",
    "\t\n",
    "\t# max_avg = max_avg / len(lines)\n",
    "\t# # print(f'max_avg: ', max_avg)\n",
    "\t# # print(f'max_avg: ', max_avg)\n",
    "\n",
    "\t# # Detects horizontal lines only\n",
    "\t# if lines is not None:\n",
    "\t# \t# print(\"Lines\", lines)\n",
    "\t# \tfor line in lines:\n",
    "\t# \t\tx1, y1, x2, y2 = line[0]\n",
    "\t\t\t\n",
    "\t# \t\t# Calculate the slope of the line\n",
    "\t# \t\tslope = (y2 - y1) / (x2 - x1 + 1e-6)  # Adding a small value to avoid division by zero\n",
    "\t\t\t\n",
    "\t# \t\t# Set a threshold to consider a line as horizontal\n",
    "\t# \t\tslope_threshold = 0.8\n",
    "\n",
    "\t# \t\t# Check if the line is approximately horizontal\n",
    "\t# \t\tif (abs(slope) < slope_threshold \n",
    "\t# \t\t\tand max(y1, y2) > max_avg - 10 # Lowest part of the frame with line \n",
    "\t# \t\t\tand max(y1, y2) > frame_height / 2): # Detect half portion of the frame only.\n",
    "\t# \t\t\t# Draw the line on the original leadFrame\n",
    "\t# \t\t\tcv2.line(leadFrame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\t\t\t# cv2.line(leadFrame, (100, 20), (50, 50), (0, 255, 0), 2)\n",
    "\n",
    "\t\t\t\n",
    "\treturn lines\n",
    "\n",
    "def harrisCorner(harrisImage):\n",
    "\t\n",
    "\tgray = cv2.cvtColor(harrisImage, cv2.COLOR_BGR2GRAY)\n",
    "# # Find Harris corners\n",
    "\tgray = np.float32(gray)\n",
    "\tdst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "\tdst = cv2.dilate(dst, None)\n",
    "\tret, dst = cv2.threshold(dst, 0.01 * dst.max(), 255, 0)\n",
    "\tdst = np.uint8(dst)\n",
    "\n",
    "# # Find centroids\n",
    "\tret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
    "\n",
    "# # Define the criteria to stop and refine the corners\n",
    "\tcriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "\tcorners = cv2.cornerSubPix(gray, np.float32(centroids), (5, 5), (-1, -1), criteria)\n",
    "\n",
    "# # Now draw them\n",
    "\tfor i in range(len(corners)):\n",
    "\t\tx, y = corners[i]\n",
    "\t\tcv2.circle(harrisImage, (int(x), int(y)), 3, (0, 150, 255), -1)\n",
    "\n",
    "\treturn harrisImage\n",
    "\n",
    "lk_params = dict(winSize=(15, 15),\n",
    "\t\t\t\t maxLevel=2,\n",
    "\t\t\t\t criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "feature_params = dict(maxCorners=20,\n",
    "\t\t\t\t\t  qualityLevel=0.3,\n",
    "\t\t\t\t\t  minDistance=10,\n",
    "\t\t\t\t\t  blockSize=7)\n",
    "\n",
    "trajectory_len = 40\n",
    "detect_interval = 5\n",
    "trajectories = []\n",
    "roi_trajectories = []\n",
    "object_max_search = 0\n",
    "frame_idx = 0\n",
    "\n",
    "# vid_path = './resources/maze/VID_20231226_124638.mp4'\n",
    "# vid_path = './resources/maze/VID_20231226_124740.mp4'\n",
    "# vid_path = './resources/maze/VID_20231226_124828.mp4'\n",
    "# vid_path = './resources/maze/VID_20231226_124843.mp4'\n",
    "# vid_path = './resources/maze/VID_20231226_124851.mp4'\n",
    "\n",
    "# vid_path = './resources/maze/from_start1_rotate.mp4'\n",
    "# vid_path = './resources/maze/start1.mp4'\n",
    "# vid_path = './resources/videos/boy-walking.mp4'\n",
    "# vid_path = './resources/maze/start1_steady.mp4'\n",
    "vid_path = './resources/maze/with_obstacle.mp4'\n",
    "# vid_path = './mazeresources//with_obstacle_steady.mp4'\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "# hog.setSVMDetector(cv2.HOGDescriptor_setSVMDetector())\n",
    "# cv2.HOGDescriptor_setSVMDetector\n",
    "\n",
    "# Create a term criteria for CAMShift\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "capCounter = 0\n",
    "\n",
    "while True:\n",
    "\tsuc, frame = cap.read()\n",
    "\t\n",
    "\tif not suc:\n",
    "\t\tprint(\"There is No Frame \")\n",
    "\t\tbreak\n",
    "\n",
    "\tframe_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tframe_graye = cv2.Canny(frame_gray, 100, 200)\n",
    "\tframe_canny_merge = frame_gray\n",
    "\tframe_hog = frame_gray.copy()\n",
    "\timg = frame #.copy()\n",
    "\n",
    "\tboxes, weights = hog.detectMultiScale(frame_hog, winStride=(8, 8), padding=(8, 8), scale=1.05)\n",
    "\n",
    "\t# Use the first detected person to initialize CAMShift tracking\n",
    "\t# if len(boxes) > 0:\n",
    "\t# \tx, y, w, h = boxes[0]\n",
    "\t# \ttrack_window = (x, y, w, h)\n",
    "\n",
    "\tfor (x, y, w, h) in boxes:\n",
    "\t\tcv2.rectangle(frame_hog, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "\t# Calculate optical flow for a sparse feature set using the iterative Lucas-Kanade Method\n",
    "\tif len(trajectories) > 0:\n",
    "\t\t# print(\"trajectories\", len(trajectories[0]))\n",
    "\n",
    "\t\timg0, img1 = prev_gray, frame_graye\n",
    "\t\tp0 = np.float32([trajectory[-1] for trajectory in trajectories]).reshape(-1, 1, 2)\n",
    "\t\t\n",
    "\t\t# calcOpticalFlowPyrLK -> optical flow-based feature tracking algorithm\n",
    "\t\tp1, _st, _err = cv2.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n",
    "\t\tp0r, _st, _err = cv2.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)\n",
    "\t\td = abs(p0 - p0r).reshape(-1, 2).max(-1)\n",
    "\t\tgood = d < 1\n",
    "\n",
    "\t\tnew_trajectories = []\n",
    "\t\tobject_trajectories = []\n",
    "\t\tisObjectDetected = False\n",
    "\n",
    "\t\t# Get all the trajectories\n",
    "\t\tfor trajectory, (x, y), good_flag in zip(trajectories, p1.reshape(-1, 2), good):\n",
    "\t\t\tif not good_flag:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ttrajectory.append((x, y))\n",
    "\t\t\tif len(trajectory) > trajectory_len:\n",
    "\t\t\t\tdel trajectory[0]\n",
    "\t\t\tnew_trajectories.append(trajectory)\n",
    "\n",
    "\t\t\t# Newest detected point\n",
    "\t\t\t# if len(object_trajectories) == 0:\n",
    "\t\t\t# \tobject_trajectories.append(trajectory)\n",
    "\n",
    "\t\t\tif object_max_search < y and len(new_trajectories) > 200:\n",
    "\t\t\t\tobject_trajectories.append(trajectory)\n",
    "\t\t\t\tprint(\"Object Detected\")\n",
    "\t\t\t\tcv2.circle(img, (int(x), int(y)), 2, (0, 0, 255), -1)\n",
    "\t\t\t\tisObjectDetected = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tisObjectDetected = False\n",
    "\n",
    "\t\t\t# print(len(trajectory))\n",
    "\t\t\t\t# print(\"Object Detected\")\n",
    "\n",
    "\t\t\t# roi_trajectories\n",
    "\n",
    "\t\ttrajectories = new_trajectories\n",
    "\t\t# trajectories = new_trajectories\n",
    "\n",
    "\t\t# Draw all the trajectories\n",
    "\t\tcv2.polylines(img, [np.int32(trajectory) for trajectory in trajectories], False, (0, 255, 0))\n",
    "\t\tcv2.putText(img, 'track count: %d' % len(trajectories), (20, 80), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "\t\tif isObjectDetected:\n",
    "\t\t\tcv2.putText(img, 'Object Detected', (20, 100), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "\t\telse: \n",
    "\t\t\tcv2.putText(img, '', (20, 100), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "\n",
    "\t# Update interval - When to update and detect new features\n",
    "\tif frame_idx % detect_interval == 0:\n",
    "\t\tmask = np.zeros_like(frame_graye)\n",
    "\t\tmask[:] = 255\n",
    "\n",
    "\t\t# Lastest point in latest trajectory\n",
    "\t\tfor x, y in [np.int32(trajectory[-1]) for trajectory in trajectories]:\n",
    "\t\t\t# if object_max_search < y:\n",
    "\t\t\tcv2.circle(mask, (x, y), 3, 0, -1) # Desc: Dots in the Screen\n",
    "\n",
    "\t\t# Detect the good features to track -> Lukas-Kanade\n",
    "\t\tp = cv2.goodFeaturesToTrack(frame_graye, mask=mask, **feature_params)\n",
    "\t\tif p is not None:\n",
    "\t\t\t# If good features can be tracked - add that to the trajectories\n",
    "\t\t\tfor x, y in np.float32(p).reshape(-1, 2):\n",
    "\t\t\t\ttrajectories.append([(x, y)])\n",
    "\n",
    "\tframe_idx += 1\n",
    "\tprev_gray = frame_graye\n",
    "\t# print(img.shape)\n",
    "\t# print(mask.shape)\n",
    "\t# print(frame_graye.shape)\n",
    "\n",
    "\t# if img.shape[2] == 3:\n",
    "\t# \timg = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "\t# █▀ █ █ █▀█ █ █ █   █▀█ █▄ █ █   █▄█   █   █▀█ █ █ █ █▀▀ █▀█   █   █▀▀ ▄▀█ █▀▄ █ █▄ █ █▀▀   █   █ █▄ █ █▀▀ █▀\n",
    "\t# ▄█ █▀█ █▄█ ▀▄▀▄▀   █▄█ █ ▀█ █▄▄  █    █▄▄ █▄█ ▀▄▀▄▀ ██▄ █▀▄   █▄▄ ██▄ █▀█ █▄▀ █ █ ▀█ █▄█   █▄▄ █ █ ▀█ ██▄ ▄█\n",
    "\tlines = detect_leading_lines(frame_canny_merge)\n",
    "\n",
    "\tlower_line_y_coordinates = []\n",
    "\tframe_height = frame_canny_merge.shape[0]\n",
    "\tmax_avg = 0\n",
    "\n",
    "\tif lines is not None:\n",
    "\t\tfor line in lines:\n",
    "\t\t\tx1, y1, x2, y2 = line[0]\n",
    "\t\t\t# print(f'max(y1, y2): {max(y1, y2)}')\n",
    "\t\t\tmax_avg += max(y1, y2)\n",
    "\t\n",
    "\tmax_avg = max_avg / len(lines)\n",
    "\t# print(f'max_avg: ', max_avg)\n",
    "\n",
    "\t# Detects horizontal lines only\n",
    "\tif lines is not None:\n",
    "\t\tfor line in lines:\n",
    "\t\t\tx1, y1, x2, y2 = line[0]\n",
    "\t\t\t\n",
    "\t\t\t# Calculate the slope of the line\n",
    "\t\t\tslope = (y2 - y1) / (x2 - x1 + 1e-6)  # Adding a small value to avoid division by zero\n",
    "\t\t\t\n",
    "\t\t\t# Set a threshold to consider a line as horizontal\n",
    "\t\t\tslope_threshold = 0.8 # 0.2 and below is strictly horizontal, added 0.8 to make it read lines that are changing rotation during turning of camera\n",
    "\n",
    "\t\t\t# Check if the line is approximately horizontal\n",
    "\t\t\tif (abs(slope) < slope_threshold \n",
    "\t\t\t\tand max(y1, y2) > max_avg - 10 # Lowest part of the frame with line \n",
    "\t\t\t\tand max(y1, y2) > frame_height / 2): # Detect half portion of the frame only.\n",
    "\t\t\t\t# Draw the line on the original leadFrame\n",
    "\t\t\t\tcv2.line(frame_canny_merge, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\t\t\t\tobject_max_search = frame_height / 2\n",
    "\n",
    "\t# harrisCornerFrame = harrisCorner(frame)\n",
    "\t# cannyImgInv = removeBlack(frame_graye)\n",
    "\n",
    "\tif img.shape[2] == 3:\n",
    "\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "\t\n",
    "\t# █▀▀ █▀█ █▄ █ █ █ █▀▀ █▀█ ▀█▀   █ █▀▄▀█ █▀▀   ▀█▀ █▀█   █▄▄ █▀▀ █▀█ ▄▀█\n",
    "\t# █▄▄ █▄█ █ ▀█ ▀▄▀ ██▄ █▀▄  █    █ █ ▀ █ █▄█    █  █▄█   █▄█ █▄█ █▀▄ █▀█ to make the merging of windows possible\n",
    "\timg = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Lukas-Kanade\n",
    "\tmask = cv2.cvtColor(mask, cv2.COLOR_BGR2BGRA) # Lukas-Kanade\n",
    "\tframe_graye = cv2.cvtColor(frame_graye, cv2.COLOR_BGR2BGRA) # Canny\n",
    "\tframe_canny_merge = cv2.cvtColor(frame_canny_merge, cv2.COLOR_BGR2BGRA) # Canny\n",
    "\tframe_hog = cv2.cvtColor(frame_hog, cv2.COLOR_BGR2BGRA) # Canny\n",
    "\n",
    "\t# For No Video Frame Only\n",
    "\tm = np.zeros_like(frame_graye)\n",
    "\tm[:] = 100\n",
    "\tprint(frame_graye.shape)\n",
    "\thh = int(frame_graye.shape[0] / 2)\n",
    "\tww = int(frame_graye.shape[1] / 2) - (5 * 35)\n",
    "\t\n",
    "\t# █▀█ █   ▄▀█ █▀▀ █▀▀   ▀█▀ █▀▀ ▀▄▀ ▀█▀   █▀▀ █▀█ █▀█   █▀▄ █▀▀ █▀ █▀▀ █▀█ █ █▀█ ▀█▀ █ █▀█ █▄ █\n",
    "\t# █▀▀ █▄▄ █▀█ █▄▄ ██▄    █  ██▄ █ █  █    █▀  █▄█ █▀▄   █▄▀ ██▄ ▄█ █▄▄ █▀▄ █ █▀▀  █  █ █▄█ █ ▀█\n",
    "\tcv2.putText(img, 'Using: Lukas-Kanade (flow-based feature tracking)', (20, 50), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 2)\n",
    "\tcv2.putText(mask, 'Using: Lukas-Kanade (flow estimation & good features tracking)', (20, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "\tcv2.putText(frame_graye, 'Using: Canny (Edge Detection)', (20, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "\tcv2.putText(frame_canny_merge, 'Using: Hough Lines (Leading Lines Detection)', (20, 50), cv2.FONT_HERSHEY_PLAIN, 1, (255, 100, 0), 2)\n",
    "\tcv2.putText(frame_hog, 'Using: HOG (People Detector)', (20, 50), cv2.FONT_HERSHEY_PLAIN, 1, (255, 100, 0), 2)\n",
    "\tcv2.putText(m, 'NO VIDEO', (ww, hh), cv2.FONT_HERSHEY_PLAIN, 5, (0, 255, 100), 2)\n",
    "\t# print(h, w)\n",
    "\t# Combine Optical Flow, , and Canny (Edge Detection)\n",
    "\t# result = np.concatenate((img, mask, frame_graye), axis=1)  # Concatenate vertically\n",
    "\t# h = int(result.shape[1] / 1) # if using hd camera, divide by 3 instead else 2\n",
    "\t# w = int(result.shape[0] / 1)\n",
    "\t# resized_image = cv2.resize(result, (h, w))\n",
    "\n",
    "\t\n",
    "\t# █▀▀ █▀█ █▀▄▀█ █▄▄ █ █▄ █ █▀▀   █▀▀ █▀█ ▄▀█ █▀▄▀█ █▀▀   ▀█▀ █▀█   █▀█ █▄ █ █▀▀   █ █ █ █ █▄ █ █▀▄ █▀█ █ █ █\n",
    "\t# █▄▄ █▄█ █ ▀ █ █▄█ █ █ ▀█ ██▄   █▀  █▀▄ █▀█ █ ▀ █ ██▄    █  █▄█   █▄█ █ ▀█ ██▄   ▀▄▀▄▀ █ █ ▀█ █▄▀ █▄█ ▀▄▀▄▀\n",
    "\ttop_row = np.concatenate((img, mask), axis=1)\n",
    "\tmiddle_row = np.concatenate((frame_graye, frame_canny_merge), axis=1)\n",
    "\tbottom_row = np.concatenate((frame_hog, m), axis=1)\n",
    "\n",
    "\t# Set the dimensions of the window you want to create\n",
    "\twindow_width = get_screen_size()[0]\n",
    "\twindow_height = get_screen_size()[1]\n",
    "\n",
    "\tcombined_frames = np.concatenate((top_row, middle_row, bottom_row), axis=0)\n",
    "\t\n",
    "\tscaling_factor = min(window_height / combined_frames.shape[1], window_width / combined_frames.shape[0])\n",
    "\th = window_width #int(combined_frames.shape[1] / 1.5) # if using hd camera, divide by 3 instead else 2\n",
    "\tw = window_height #int(combined_frames.shape[0] / 1.5)\n",
    "\t# resized_image = cv2.resize(combined_frames, (h, w))\n",
    "\tresized_image = cv2.resize(combined_frames, (int(combined_frames.shape[1] * scaling_factor), int(combined_frames.shape[0] * scaling_factor)))\n",
    "\n",
    "\theight, width = resized_image.shape[:2]\n",
    "\t\n",
    "\tx_position = int(abs(width - window_width) / 2)\n",
    "\ty_position = int(abs(height - window_height) / 2) - 30 #int(abs(height - window_height))\n",
    "\n",
    "\t\n",
    "\t# █▀ █ █ █▀█ █ █ █   █ █ █ █ █▄ █ █▀▄ █▀█ █ █ █\n",
    "\t# ▄█ █▀█ █▄█ ▀▄▀▄▀   ▀▄▀▄▀ █ █ ▀█ █▄▀ █▄█ ▀▄▀▄▀\n",
    "\t# Show Results, \n",
    "\tcv2.imshow('Merged Frames', resized_image)\n",
    "\tcv2.moveWindow('Merged Frames', x_position, y_position)\n",
    "\t# cv2.imshow('Mask', mask)\n",
    "\n",
    "\tif cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
